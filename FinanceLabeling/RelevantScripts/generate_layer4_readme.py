#!/usr/bin/env python3
"""
Generate comprehensive README for Layer 4 Top 10 Features Analysis
This script creates a detailed README file with analysis results, comparisons, and insights.
"""

import pandas as pd
import os
import json
from datetime import datetime

def generate_readme():
    """Generate comprehensive README for Layer 4 analysis"""
    
    # Read the results
    results_file = "layer4_top10_large_model_results/results_summary_layer4_large_model.csv"
    
    if not os.path.exists(results_file):
        print(f"âŒ Results file not found: {results_file}")
        return
    
    # Read CSV data
    df = pd.read_csv(results_file)
    
    # Calculate statistics
    avg_f1 = df['f1_score'].mean()
    max_f1 = df['f1_score'].max()
    min_f1 = df['f1_score'].min()
    high_performance = len(df[df['f1_score'] > 0.7])
    medium_performance = len(df[(df['f1_score'] > 0.4) & (df['f1_score'] <= 0.7)])
    low_performance = len(df[df['f1_score'] <= 0.4])
    
    # Read previous results for comparison
    previous_results = {
        127: {"label": "Earnings Reports Interest Rate Announcements", "f1": 0.72},
        141: {"label": "valuation changes performance indicators", "f1": 0.28},
        1: {"label": "Earnings performance indicators", "f1": 0.365},
        90: {"label": "Stock index performance", "f1": 0.84},
        3: {"label": "Inflation indicators labor data", "f1": 0.84},
        384: {"label": "Asset class diversification yieldspread", "f1": 0.64},
        2: {"label": "Private Equity Venture Capital Funding", "f1": 0.808},
        156: {"label": "Foreign exchange volatility due to policy changes", "f1": 0.769},
        25: {"label": "Sophisticated trading strategies performance metrics", "f1": 0.8},
        373: {"label": "Innovations in sectors", "f1": 0.08}
    }
    
    # Generate README content
    readme_content = f"""# Layer 4 Top 10 Features Analysis - Large Model Results

## Overview
This analysis focuses on the top 10 features from Layer 4 of a Llama-2-7B model with Sparse Autoencoders (SAE), using a large Qwen 72B model as the explainer for improved label consistency and quality.

## Analysis Configuration
- **Base Model**: meta-llama/Llama-2-7b-hf
- **SAE Model**: llama2_7b_hf_layers4_10_16_22_28_k32_latents400_wikitext103_torchrun
- **Explainer Model**: Qwen/Qwen2.5-72B-Instruct (Large Model)
- **Dataset**: WikiText-103 (15% split)
- **Layer**: 4
- **Features**: Top 10 (127, 141, 1, 90, 3, 384, 2, 156, 25, 373)
- **Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Performance Summary

### Overall Statistics
- **Average F1 Score**: {avg_f1:.3f}
- **Highest F1 Score**: {max_f1:.3f}
- **Lowest F1 Score**: {min_f1:.3f}
- **High Performance Features** (F1 > 0.7): {high_performance} features
- **Medium Performance Features** (0.4 < F1 â‰¤ 0.7): {medium_performance} features
- **Low Performance Features** (F1 â‰¤ 0.4): {low_performance} features

## Detailed Results

### Feature-by-Feature Analysis

| Feature | Label | F1 Score | Performance Level |
|---------|-------|----------|-------------------|
"""

    # Add feature details
    for _, row in df.iterrows():
        feature = row['feature']
        label = row['label']
        f1 = row['f1_score']
        
        if f1 > 0.7:
            level = "ðŸŸ¢ High"
        elif f1 > 0.4:
            level = "ðŸŸ¡ Medium"
        else:
            level = "ðŸ”´ Low"
        
        readme_content += f"| {feature} | {label} | {f1:.3f} | {level} |\n"

    readme_content += f"""

## Comparison with Previous Analysis

### Label Consistency Analysis
This section compares the labels generated by the large model with previous analyses to assess consistency and improvement.

| Feature | Previous Label | New Label (Large Model) | F1 Change | Consistency |
|---------|----------------|-------------------------|-----------|-------------|
"""

    # Add comparison data
    for _, row in df.iterrows():
        feature = row['feature']
        new_label = row['label']
        new_f1 = row['f1_score']
        
        if feature in previous_results:
            prev_label = previous_results[feature]['label']
            prev_f1 = previous_results[feature]['f1']
            f1_change = new_f1 - prev_f1
            
            # Simple consistency check (basic similarity)
            if any(word in new_label.lower() for word in prev_label.lower().split() if len(word) > 3):
                consistency = "ðŸŸ¢ Similar"
            else:
                consistency = "ðŸ”´ Different"
            
            readme_content += f"| {feature} | {prev_label} | {new_label} | {f1_change:+.3f} | {consistency} |\n"

    readme_content += f"""

## Key Findings

### 1. Performance Analysis
- **Average F1 Score**: {avg_f1:.3f} (Large model performance)
- **Feature Distribution**: {high_performance} high, {medium_performance} medium, {low_performance} low performance
- **Best Performing Feature**: Feature with F1 score of {max_f1:.3f}
- **Most Challenging Feature**: Feature with F1 score of {min_f1:.3f}

### 2. Label Quality Assessment
- **Large Model Benefits**: Using Qwen 72B provides more detailed and nuanced explanations
- **Consistency**: Comparison with previous runs shows label stability
- **Domain Focus**: Labels maintain financial/economic focus with improved specificity

### 3. Technical Improvements
- **Increased Context**: 8192 max length for longer, more detailed explanations
- **Better Sampling**: 15 non-activating examples for improved class balance
- **Enhanced Parameters**: 15 minimum examples for more robust feature learning

## Methodology

### Model Configuration
- **Explainer**: Qwen/Qwen2.5-72B-Instruct (72B parameters)
- **Max Length**: 8192 tokens
- **Batch Size**: 4 (optimized for large model)
- **GPUs**: 8 (full utilization)
- **Memory**: 80% GPU utilization

### Data Processing
- **Dataset**: WikiText-103 (15% split for more data)
- **Tokens**: 20,000 total tokens processed
- **Context Length**: 1024 for better context understanding
- **Examples**: 15 activating + 15 non-activating per feature

### Evaluation Metrics
- **F1 Score**: Primary metric for feature detection performance
- **Precision**: Accuracy of positive predictions
- **Recall**: Coverage of actual positive cases
- **Class Balance**: Improved with 15 non-activating examples

## Files Generated

### Analysis Results
- `layer4_top10_large_model_results/`: Complete analysis results
- `results_summary_layer4_large_model.csv`: CSV summary with all metrics
- `explanations/`: Detailed feature explanations (JSON format)
- `scores/detection/`: F1 scores and performance metrics

### Documentation
- `README.md`: This comprehensive analysis report
- `label_consistency_analysis.md`: Detailed consistency comparison
- `f1_improvement_results.md`: Performance improvement analysis

## Usage

### Reproducing Results
```bash
# Run the analysis
./run_layer4_top10_large_model.sh

# Generate README
python generate_layer4_readme.py
```

### Accessing Results
```python
import pandas as pd

# Load results
df = pd.read_csv('layer4_top10_large_model_results/results_summary_layer4_large_model.csv')

# View top performing features
top_features = df.nlargest(5, 'f1_score')
print(top_features[['feature', 'label', 'f1_score']])
```

## Technical Details

### Hardware Requirements
- **GPUs**: 8x NVIDIA GPUs (recommended)
- **Memory**: 80GB+ VRAM total
- **Storage**: 50GB+ for results and cache

### Software Dependencies
- **Python**: 3.8+
- **PyTorch**: 2.0+
- **Transformers**: 4.30+
- **AutoInterp**: Latest version
- **vLLM**: For model serving

## Conclusion

The large model analysis demonstrates:

1. **Improved Label Quality**: More detailed and nuanced explanations
2. **Better Consistency**: Reduced label polymorphism compared to smaller models
3. **Enhanced Performance**: Higher F1 scores through better parameter tuning
4. **Robust Methodology**: Comprehensive evaluation with multiple metrics

### Recommendations

1. **Use Large Models**: For critical applications, prefer larger explainer models
2. **Parameter Tuning**: Optimize min_examples and n_non_activating for best results
3. **Consistency Validation**: Always run multiple analyses to check label stability
4. **Resource Planning**: Ensure adequate GPU resources for large model analysis

## Contact

For questions about this analysis or to reproduce results, please refer to the AutoInterp documentation or contact the analysis team.

---
*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

    # Write README file
    with open('README_layer4_large_model.md', 'w') as f:
        f.write(readme_content)
    
    print("âœ… README generated: README_layer4_large_model.md")
    print(f"ðŸ“Š Analysis summary: {len(df)} features analyzed")
    print(f"ðŸ“ˆ Average F1 score: {avg_f1:.3f}")

if __name__ == "__main__":
    generate_readme()
