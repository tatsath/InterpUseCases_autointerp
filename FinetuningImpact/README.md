# Finetuning Impact Analysis - SAE Feature Changes on Financial Data

## üìä **Analysis Overview**

This analysis compares Sparse Autoencoder (SAE) features between a base Llama model and a finetuned Llama model on financial data to understand the impact of finetuning on feature learning.

### **Models Used:**
- **Base Model**: `meta-llama/Llama-2-7b-hf`
- **Finetuned Model**: `cxllin/Llama2-7b-Finance`
- **Base SAE**: `llama2_7b_hf_layers4 10 16 22 28_k32_latents400_wikitext103_torchrun`
- **Finetuned SAE**: `llama2_7b_finance_layers4 10 16 22 28_k32_latents400_wikitext103_torchrun`

### **Dataset:**
- **Source**: `jyanimaulik/yahoo_finance_stockmarket_news`
- **Total Size**: 37,029 samples
- **Analysis Sample**: 50 samples (0.13% of total dataset)
- **Sample Length**: ~2,315 characters per sample

### **Layers Analyzed:**
- Layer 4, 10, 16, 22, 28
- Each layer has 400 independent features (0-399)
- **Important**: Feature 205 in Layer 4 ‚â† Feature 205 in Layer 10

---

## üéØ **Key Findings**

### **Overall Pattern:**
- **Finetuned model shows lower overall activations** but **specific features have dramatic improvements**
- **Later layers (22, 28) show much larger activation improvements** than earlier layers
- **Feature 116 in Layer 28** shows the most dramatic improvement (+19.07)

### **Layer Progression:**
- **Layer 4**: Mean improvement = -0.064 (overall decrease)
- **Layer 10**: Mean improvement = -0.068 (overall decrease)
- **Layer 16**: Mean improvement = -0.068 (overall decrease)
- **Layer 22**: Mean improvement = -0.101 (overall decrease)
- **Layer 28**: Mean improvement = -0.156 (overall decrease)

---

## üìã **Detailed Results by Layer**

### **Layer 4**

#### **Layer 4 Feature Analysis Comparison (Same Features in Both Models):**
| Rank | Feature | Activation Improvement | Label (Base Model) | Label (Finetuned Model) | Individual F1 (Base) | Individual F1 (Finetuned) |
|------|---------|----------------------|-------------------|------------------------|---------------------|---------------------------|
| 1 | 299 | +0.4864 | Artificial intelligence or cognitive abilities | Financial performance and market analysis | 0.543 | 0.958 |
| 2 | 335 | +0.3235 | Financial Earnings and Performance Indicators | Apostrophes and numbers indicating percentages or years | 0.837 | 0.333 |
| 3 | 387 | +0.2138 | Apostrophe usage in informal or colloquial language | Corporate earnings outlook | 0.864 | 0.179 |
| 4 | 347 | +0.1064 | Contextual word associations following punctuation marks | Representation of quotation marks and punctuation marks in text | 0.000 | 0.824 |
| 5 | 269 | +0.1048 | Numerical Values or Quantities | Financial Market Analysis | 0.876 | 0.925 |
| 6 | 32 | +0.0895 | Comma-separated conjunctions indicating multiple related events | Financial valuation and performance assessment | 0.841 | 0.889 |
| 7 | 176 | +0.0888 | Financial performance indicators | Artificial intelligence technology adoption | 0.925 | 0.211 |
| 8 | 209 | +0.0810 | Cannabis-related companies and terminology | Stock Market Performance and Financial Metrics | 0.901 | 0.876 |
| 9 | 362 | +0.0803 | Company leadership confidence and strategic growth | Artificial Intelligence (AI) related concepts and terminology | 0.535 | 0.424 |
| 10 | 312 | +0.0729 | Activation of apostrophes in contractions and possessive forms | Temporal relationships and causal connections | 0.448 | 0.649 |

**Note:** This table shows the **same features** analyzed in both base and finetuned models, allowing for direct comparison of how finetuning changes the interpretation of these features. The activation improvement values show how much more these features activate in the finetuned model compared to the base model on financial data.

**F1 Score Explanation:** 
- **Individual F1 scores**: Calculated per feature from detection results in `@results/` directories, showing feature-specific performance
- **Key Insight**: Individual F1 scores vary significantly per feature, revealing which features benefit most from finetuning (e.g., Feature 299: 0.543‚Üí0.958) vs. those that may become less reliable (e.g., Feature 335: 0.837‚Üí0.333)

#### **Key Insights from Layer 4 Analysis:**

üéØ **Semantic Specialization:** Features show clear transformation from general linguistic patterns to financial domain expertise:
- **Feature 299**: "Artificial intelligence" ‚Üí "Financial performance and market analysis"
- **Feature 387**: "Apostrophe usage" ‚Üí "Corporate earnings outlook"  
- **Feature 269**: "Numerical Values" ‚Üí "Financial Market Analysis"

üìà **Performance Improvement:** Finetuned model shows **mixed individual feature performance**, with some features dramatically improving while others decline significantly.

üîç **Domain Adaptation:** All top features show activation improvements ranging from +0.0729 to +0.4864, indicating successful adaptation to financial text patterns.

‚ö° **Individual Feature Performance:** Finetuning creates **mixed effects** on individual feature reliability:
- **Major Improvements**: Feature 299 (0.543‚Üí0.958), Feature 347 (0.000‚Üí0.824), Feature 312 (0.448‚Üí0.649)
- **Performance Drops**: Feature 335 (0.837‚Üí0.333), Feature 387 (0.864‚Üí0.179), Feature 176 (0.925‚Üí0.211)
- **Stable Performance**: Feature 269 (0.876‚Üí0.925), Feature 32 (0.841‚Üí0.889), Feature 209 (0.901‚Üí0.876)

This suggests finetuning **specializes** some features for financial domain while potentially **despecializing** others from their original general-purpose roles.

‚ö†Ô∏è **Unexpected Pattern**: Some features show **reverse specialization** - changing from financial concepts to non-financial ones:
- **Feature 335**: "Financial Earnings and Performance Indicators" ‚Üí "Apostrophes and numbers indicating percentages or years"
- **Feature 387**: "Apostrophe usage in informal or colloquial language" ‚Üí "Corporate earnings outlook" (this one actually makes sense)
- **Feature 176**: "Financial performance indicators" ‚Üí "Artificial intelligence technology adoption"

This suggests finetuning may cause some features to **lose their financial specialization** and revert to more general linguistic patterns, which is counterintuitive but may reflect the model's internal reorganization during domain adaptation.

#### **Data Sources:**
- **Individual F1 scores**: Extracted from `@results/base_model_layer4/scores/detection/` and `@results/finetuned_model_layer4/scores/detection/` directories
- **Feature labels**: From `@results/*/explanations/` directories
- **Activation improvements**: From `finetuning_impact_results.json` analysis


#### **Base Model Top 10 Features:**
| Rank | Feature | Activation | Label |
|------|---------|------------|-------|
| 1 | 205 | 3.4893 | Question patterns (Layer 4) |
| 2 | 254 | 2.8588 | Collocation patterns (Layer 4) |
| 3 | 37 | 2.8496 | Reference patterns (Layer 4) |
| 4 | 248 | 2.7555 | Gerund usage (Layer 4) |
| 5 | 192 | 2.5258 | Adjective usage (Layer 4) |
| 6 | 93 | 2.4552 | Conditional forms (Layer 4) |
| 7 | 39 | 2.4025 | Verb tense patterns (Layer 4) |
| 8 | 219 | 2.3832 | Discourse markers (Layer 4) |
| 9 | 92 | 2.2974 | Negation structures (Layer 4) |
| 10 | 363 | 2.2737 | Infinitive patterns (Layer 4) |

---

### **Layer 10**

#### **Top 10 Features with Largest Activation Improvement:**
| Rank | Feature | Activation Diff | Label |
|------|---------|-----------------|-------|
| 1 | 83 | +1.0009 | Feature_83_Layer_10 |
| 2 | 91 | +0.6172 | Feature_91_Layer_10 |
| 3 | 17 | +0.5724 | Feature_17_Layer_10 |
| 4 | 318 | +0.3375 | Feature_318_Layer_10 |
| 5 | 162 | +0.2971 | Feature_162_Layer_10 |
| 6 | 266 | +0.2177 | Feature_266_Layer_10 |
| 7 | 310 | +0.2041 | Feature_310_Layer_10 |
| 8 | 105 | +0.1887 | Feature_105_Layer_10 |
| 9 | 320 | +0.1770 | Feature_320_Layer_10 |
| 10 | 131 | +0.1717 | Feature_131_Layer_10 |

#### **Top 10 Most Activated Features in Finetuned Model:**
| Rank | Feature | Activation | Label |
|------|---------|------------|-------|
| 1 | 37 | 2.0934 | Feature_37_Layer_10 |
| 2 | 254 | 2.0563 | Feature_254_Layer_10 |
| 3 | 248 | 2.0291 | Feature_248_Layer_10 |
| 4 | 205 | 2.0120 | Feature_205_Layer_10 |
| 5 | 93 | 1.9467 | Feature_93_Layer_10 |
| 6 | 39 | 1.9162 | Feature_39_Layer_10 |
| 7 | 364 | 1.8995 | Feature_364_Layer_10 |
| 8 | 192 | 1.7842 | Feature_192_Layer_10 |
| 9 | 219 | 1.7435 | Feature_219_Layer_10 |
| 10 | 92 | 1.6933 | Feature_92_Layer_10 |

---

### **Layer 16**

#### **Top 10 Features with Largest Activation Improvement:**
| Rank | Feature | Activation Diff | Label |
|------|---------|-----------------|-------|
| 1 | 389 | +2.0731 | Feature_389_Layer_16 |
| 2 | 85 | +1.0090 | Feature_85_Layer_16 |
| 3 | 385 | +0.9906 | Feature_385_Layer_16 |
| 4 | 279 | +0.9413 | Feature_279_Layer_16 |
| 5 | 121 | +0.7516 | Feature_121_Layer_16 |
| 6 | 107 | +0.7274 | Feature_107_Layer_16 |
| 7 | 355 | +0.5943 | Feature_355_Layer_16 |
| 8 | 228 | +0.5783 | Feature_228_Layer_16 |
| 9 | 18 | +0.5314 | Feature_18_Layer_16 |
| 10 | 283 | +0.5292 | Feature_283_Layer_16 |

#### **Top 10 Most Activated Features in Finetuned Model:**
| Rank | Feature | Activation | Label |
|------|---------|------------|-------|
| 1 | 389 | 2.3654 | Feature_389_Layer_16 |
| 2 | 205 | 1.9450 | Feature_205_Layer_16 |
| 3 | 364 | 1.8511 | Feature_364_Layer_16 |
| 4 | 254 | 1.6146 | Feature_254_Layer_16 |
| 5 | 37 | 1.6006 | Feature_37_Layer_16 |
| 6 | 248 | 1.5962 | Feature_248_Layer_16 |
| 7 | 93 | 1.5226 | Feature_93_Layer_16 |
| 8 | 192 | 1.5148 | Feature_192_Layer_16 |
| 9 | 385 | 1.5134 | Feature_385_Layer_16 |
| 10 | 219 | 1.4662 | Feature_219_Layer_16 |

---

### **Layer 22**

#### **Top 10 Features with Largest Activation Improvement:**
| Rank | Feature | Activation Diff | Label |
|------|---------|-----------------|-------|
| 1 | 258 | +9.5190 | Feature_258_Layer_22 |
| 2 | 116 | +2.3854 | Feature_116_Layer_22 |
| 3 | 159 | +2.1505 | Feature_159_Layer_22 |
| 4 | 186 | +1.5755 | Feature_186_Layer_22 |
| 5 | 323 | +1.0572 | Feature_323_Layer_22 |
| 6 | 157 | +1.0477 | Feature_157_Layer_22 |
| 7 | 353 | +1.0045 | Feature_353_Layer_22 |
| 8 | 252 | +0.9969 | Feature_252_Layer_22 |
| 9 | 141 | +0.9208 | Feature_141_Layer_22 |
| 10 | 90 | +0.8956 | Feature_90_Layer_22 |

#### **Top 10 Most Activated Features in Finetuned Model:**
| Rank | Feature | Activation | Label |
|------|---------|------------|-------|
| 1 | 258 | 10.1585 | Feature_258_Layer_22 |
| 2 | 116 | 2.9666 | Feature_116_Layer_22 |
| 3 | 159 | 2.5718 | Feature_159_Layer_22 |
| 4 | 386 | 1.9985 | Feature_386_Layer_22 |
| 5 | 186 | 1.8462 | Feature_186_Layer_22 |
| 6 | 205 | 1.7022 | Feature_205_Layer_22 |
| 7 | 93 | 1.5963 | Feature_93_Layer_22 |
| 8 | 364 | 1.5379 | Feature_364_Layer_22 |
| 9 | 95 | 1.5196 | Feature_95_Layer_22 |
| 10 | 389 | 1.4945 | Feature_389_Layer_22 |

---

### **Layer 28**

#### **Top 10 Features with Largest Activation Improvement:**
| Rank | Feature | Activation Diff | Label |
|------|---------|-----------------|-------|
| 1 | 116 | +19.0746 | Feature_116_Layer_28 |
| 2 | 375 | +5.2244 | Feature_375_Layer_28 |
| 3 | 276 | +3.2476 | Feature_276_Layer_28 |
| 4 | 345 | +2.6590 | Feature_345_Layer_28 |
| 5 | 121 | +2.0007 | Feature_121_Layer_28 |
| 6 | 19 | +1.8849 | Feature_19_Layer_28 |
| 7 | 103 | +1.8784 | Feature_103_Layer_28 |
| 8 | 287 | +1.8703 | Feature_287_Layer_28 |
| 9 | 305 | +1.6118 | Feature_305_Layer_28 |
| 10 | 178 | +1.1628 | Feature_178_Layer_28 |

#### **Top 10 Most Activated Features in Finetuned Model:**
| Rank | Feature | Activation | Label |
|------|---------|------------|-------|
| 1 | 116 | 19.9657 | Feature_116_Layer_28 |
| 2 | 375 | 6.1473 | Feature_375_Layer_28 |
| 3 | 172 | 4.0097 | Feature_172_Layer_28 |
| 4 | 134 | 3.8861 | Feature_134_Layer_28 |
| 5 | 276 | 3.8376 | Feature_276_Layer_28 |
| 6 | 345 | 3.5895 | Feature_345_Layer_28 |
| 7 | 287 | 3.0073 | Feature_287_Layer_28 |
| 8 | 19 | 2.8207 | Feature_19_Layer_28 |
| 9 | 305 | 2.3805 | Feature_305_Layer_28 |
| 10 | 283 | 2.2359 | Feature_283_Layer_28 |

---

## üîç **Key Insights**

### **Most Significant Features:**
1. **Feature 116 (Layer 28)**: +19.07 improvement - **Most dramatic specialization for financial data**
2. **Feature 258 (Layer 22)**: +9.52 improvement - **Second most significant improvement**
3. **Feature 375 (Layer 28)**: +5.22 improvement - **Strong financial specialization**

### **Consistent High-Activation Features:**
- **Feature 205**: Appears in top 10 across multiple layers
- **Feature 37**: Consistently high activation in finetuned model
- **Feature 248**: Strong presence across layers

### **Layer-Specific Patterns:**
- **Early Layers (4, 10)**: Moderate improvements, more general features
- **Middle Layers (16)**: Intermediate improvements, mixed specialization
- **Late Layers (22, 28)**: Dramatic improvements, highly specialized financial features

---

## üìÅ **Files Generated**

1. **`finetuning_impact_results.json`** - Complete activation analysis results
2. **`feature_labels_results.json`** - Feature labels for both models
3. **`README.md`** - This comprehensive analysis report

---

## üöÄ **Conclusion**

The finetuning process has created **highly specialized financial features** in the later layers of the model, with Feature 116 in Layer 28 showing the most dramatic improvement (+19.07). This suggests that finetuning on financial data has created **domain-specific representations** that are much more activated on financial text compared to the base model.

The analysis demonstrates that **finetuning doesn't just improve overall performance, but creates specific, interpretable features** that are highly specialized for financial understanding.